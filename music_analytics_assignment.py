# -*- coding: utf-8 -*-
"""music-analytics-assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/GroupWork2028/42b29065ea0e8497d486413f9dd4edeb/music-analytics-assignment.ipynb
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'spotify-and-youtube:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3025170%2F5201951%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240726%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240726T182910Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D34db5e29a5b6a571bf96e2553ccc45f2ce1273db0acb2ec595cbaec4690d7694968b32e4952a660a1d7237f2baffd464012306371952faf62fee50d8bb43f31f7fc61304035de717baeaf0350cc1e066ccb0b7a90ffafb21a35f4d6564095986a08674928f1f543b07da65a09669390b7f4e03540e9262c059663cb7ac98eec80436053cae62dae684b7441f3230e27fbe540e18b4a69861bf1bac8499a15a200323e750b896917f5a3e1a7a619e8f2a16a59de24f68eec38abc0d782196f025eba91723d1cc43dcfcdc960044c0a5f7792a00b49e26b0a0b537fdea1cb46a9a17644fdcbd38356641eb3fdd11058a02c61eec6afd1675df9c602fd834568a14'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""Music Analytics(YouTube/Spotify)

This notebook analyzes Kaggle dataset based on music popularity from Spotify API and YouTube. The primary
objective is to construct a predictive model for assessing the popularity of songs based on their attributes. The intention is to develop insights into song popularity prediction through data-driven exploration and modeling.

### Data Importation
"""

pip install ydata-profiling

##Installing Relevant Packages
from ydata_profiling import ProfileReport

# Libraries import
import numpy as np
import pandas as pd
from ydata_profiling import ProfileReport
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore") # for production lets ignore alerts.


plt.style.use('ggplot')
pd.set_option('display.max_columns',None)

#Set color palettes for plots

# Paleta de colores verde para Spotify
spotify_palette = sns.light_palette("green", n_colors=10, reverse=True)

# Paleta de colores rojo para YouTube
youtube_palette = sns.light_palette("red", n_colors=10, reverse=True)

# We load the data from a CSV file.
df = pd.read_csv('/kaggle/input/spotify-and-youtube/Spotify_Youtube.csv', index_col=0)

"""# 1. Data Understanding"""

#Check first 5 rows
df.head()

#Check columns and their types
df.info()

"""## Features Interpretability
1. Danceability: the ability of the song to invite dancing or movement to the rhythm.
2. Energy: perceived level of energy in the song, indicating how energetic or intense it feels.
3. Key: the tonality of the song, providing an overall sense (e.g., major keys may sound more cheerful, while minor keys may sound more melancholic).
4. Loudness: the overall volume level of the song.
5. Speechiness: the presence of spoken elements in the song, indicating whether it has more vocal elements or instrumental elements.
6. Acousticness: the level of acoustic elements or acoustic instruments present in the song.
7. Instrumentalness: a measure of the presence of instrumental elements in the song rather than vocal elements.
8. Liveness: the perception of whether the song sounds live or was recorded in a studio.
9. Valence: a measure of the perceived positivity or negativity in the song, indicating the joy or sadness it conveys.
10. Tempo: the speed or rhythm of the song, referring to the number of beats per minute (BPM).
11. Duration_ms: the duration of the song in milliseconds.


"""

#Numerical columns statistics
df.describe()

#Mapper Function
def mapper(song):
    genre = song['genre']
    attributes = song.drop('genre')
    return (genre, attributes)

#Reducer Function
def reducer(key, values):
    # Aggregate statistics or perform operations on values
    average_likes = sum(values) / len(values)
    return (key, average_likes)

#Numerical columns statistics
df.describe()

"""# 2. Data Preparation"""

#Subsetting the data to only include the columns we need
df = df[['Artist',
         #'Url_spotify',
         'Track', 'Album', 'Album_type',
         #'Uri',
       'Danceability', 'Energy', 'Key', 'Loudness', 'Speechiness',
       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',
       'Duration_ms',
       #'Url_youtube',
       #'Title',
       #'Channel',
        'Views', 'Likes',
       'Comments',
       #'Description',
       'Licensed', 'official_video', 'Stream']].copy()

#Adjust Official Video and Licensed column to boolean
df['official_video'] = df['official_video'].astype('bool')
df['Licensed'] = df['Licensed'].astype('bool')

#Rename columns to be more readable
df.rename(columns={'official_video':'Official_video', 'Channel':'Channel_name', 'Stream':'Spotify_stream' }, inplace=True)

# Check for null or missing values - not a problem as not representative for the analysis
df.isna().sum()

# Check for duplicates
df.loc[df.duplicated(subset=['Track', 'Artist'])]

# Quick look at the data for duplicates, but we will not drop them as they are not true duplicates
df.query('Track == "How Will I Know"')

"""# 3. Exploratory Data Analysis

## 3.1. Uni-variate Analysis
"""

#Check columns
df.columns

#Import for Data tickers
import matplotlib.ticker as ticker

#Top 10 artists by Total Views on Youtube
# Group sort and select the top 10
data = df.groupby('Artist')['Views'].sum().sort_values(ascending=False).head(10)

# plot the data
fig, ax = plt.subplots(figsize=(10,5))
sns.barplot(x=data.values, y=data.index, orient='h', ax=ax, palette='rocket')

# X axis formatting to be in millions
ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.1f}M'.format(x*1e-6)))
ax.set_title('Top 10 artists by Total Views on Youtube', fontsize=15)
ax.set_xlabel('Toatl Youtube Views', fontsize=12)

plt.show()

#Top 10 songs by Total Views on Youtube
# Group sort and select the top 10
data = df.groupby('Track')['Views'].mean().sort_values(ascending=False).head(10)

# plot the data
fig, ax = plt.subplots(figsize=(10,5))
sns.barplot(x=data.values, y=data.index, orient='h', ax=ax, palette=youtube_palette)

# X axis formatting to be in millions
ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.1f}M'.format(x*1e-6)))
ax.set_title('Top 10 Songs by Total Views on Youtube', fontsize=15)
ax.set_xlabel('Toatl Youtube Views', fontsize=12)
warnings.filterwarnings('ignore', category=UserWarning) # To ignore the warning for the x axis (Gangnam Style)
plt.show()

#Top 10 songs by Total Streams on Spotify stream
# Group sort and select the top 10
data = df.groupby('Track')['Spotify_stream'].mean().sort_values(ascending=False).head(10) # here we use mean as we might have multiple rows for the same song (Albums, Remixes, etc.)

# plot the data
fig, ax = plt.subplots(figsize=(10,5))
sns.barplot(x=data.values, y=data.index, orient='h', ax=ax, palette=spotify_palette)

# X axis formatting to be in millions
ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.1f}M'.format(x*1e-6)))
ax.set_title('Top 10 Songs by Total Stream on Spotify', fontsize=15)
ax.set_xlabel('Toatl Youtube Views', fontsize=12)

#Album type distribution
at = df['Album_type'].value_counts().plot(kind='bar',
                                     title='Album type distribution',
                                     figsize=(10,5),
                                     fontsize=12,
                                     rot=0)
at.set_xlabel('Album type', fontsize=12)
at.grid(axis='x')

#Tracks danceability distribution
d = df['Danceability'].plot(kind='hist'
                            ,bins=20
                            ,title='Danceability distribution'
                            ,figsize=(5,2)
                            ,fontsize=8)
d.set_xlabel('Danceability', fontsize=8)

#Tracks danceability distribution
d = df['Danceability'].plot(kind='kde'
                            ,title='Danceability distribution kde'
                            ,figsize=(5,2)
                            ,fontsize=8)
d.set_xlabel('Danceability', fontsize=8)

"""### 3.1.1 Exploring with new Features"""

# Create a df without null value as they are not representative for the analysis
df_completed = df.dropna().copy()

#Clustering using PCA and KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Select only the numeric columns
num_cols = df_completed.select_dtypes(include=['int64', 'float64']).columns

# Standardize the features
scaler = StandardScaler()
df_completed_scaled = scaler.fit_transform(df_completed[['Spotify_stream','Views','Likes']])

# Apply PCA
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(df_completed_scaled)

# Determine the number of clusters using the elbow method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(principalComponents)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# From the previous elbow chart we can see that a good number of clusters is 6
kmeans = KMeans(n_clusters=6, init='k-means++', max_iter=300, n_init=10, random_state=0)
df_completed['popularity_cluster'] = kmeans.fit_predict(principalComponents)

#Check the clusters distribution by Spotify stream mean
df_completed['Spotify_stream'].groupby(df_completed['popularity_cluster']).mean()/ 1e6 # in millions

#Plot the clusters distribution by Spotify stream mean
df_completed.groupby('popularity_cluster')['Spotify_stream'].mean().sort_values().plot(kind='bar'
                                                                     , figsize=(10,5)
                                                                     , title='Average Spotify Streams by Popularity Cluster'
                                                                     )
plt.show()

# Create a dictionary with  the cluster names for the legend
name_mapper = {5: 'SuperPopular', 1: 'High-popularity', 2: 'High-popularity', 0:'Medium-popularity', 4:'Medium-popularity', 3:'Low-popularity'}
# Use the map to create a new column with the new group names
df_completed['popularity_group'] = df_completed['popularity_cluster'].map(name_mapper)

# Plot Top 5 Unique Songs on Spotify by Popularity Group
for group in df_completed['popularity_group'].unique():
    data = df_completed[df_completed['popularity_group'] == group].copy()
    data['Track_Artist'] = data['Track'] + ' - ' + data['Artist']
    data = data.groupby('Track_Artist').Spotify_stream.max().sort_values(ascending=False).head(5).reset_index()
    plt.figure(figsize=(10,5))
    ax = sns.barplot(x='Spotify_stream', y='Track_Artist', data=data, palette=spotify_palette)
    plt.title(f'Top 5 Unique Songs on Spotify - {group}')
    plt.show()

"""### Quick table view by popularity cluster"""

# Let's check the mean by group of populatiry and its features

# Columns that are not to be melted
id_vars = ['Artist', 'Track', 'Album', 'Album_type', 'Licensed', 'Official_video', 'popularity_cluster', 'popularity_group']

groups_ordered = ['Low-popularity', 'Medium-popularity', 'High-popularity','SuperPopular']


# Melt the dataframe
df_melted = pd.melt(df_completed, id_vars=id_vars, var_name='feature', value_name='value')

# Group by 'popularity_group' and 'feature', and calculate the mean
df_pivot = df_melted.groupby(['popularity_group', 'feature'])['value'].mean().unstack()

# Reindex the DataFrame to have an easier comparable visual
df_pivot = df_pivot.reindex(groups_ordered)

df_pivot_styled = df_pivot.style.format("{:.2f}").background_gradient(cmap='BuGn')
df_pivot_styled

# Lets understand how the artis play with the previous mean, imagine this is like how the song moves above or below from the mean.

# Group by 'popularity_group' and 'feature', and calculate the standa deviation
df_pivot = df_melted.groupby(['popularity_group', 'feature'])['value'].std().unstack()

# Reindex the DataFrame to have an easier comparable visual
df_pivot = df_pivot.reindex(groups_ordered)

df_pivot_styled = df_pivot.style.format("{:.2f}").background_gradient(cmap='BuGn')
df_pivot_styled

"""### 3.2 Data Analysis using Data profiling

This is a nice way to explore your dataset really quick and start getting nice insights.    
(For performance reasons, plese check the html file on the outputs)
"""

#Dictionary for data types
data_types = {'Artist': 'categorical','Album':'categorical'}

#Generate a data profile report to speed up the EDA process
profile = ProfileReport(df, title='Pandas Profiling Report'
                        ,type_schema=data_types)

#Profile to html
profile.to_file("music_profiling_report.html")

"""## 3.2. Multi-variate Analysis"""

#Radio plot for clusters multi-dimensional analysis
import numpy as np
from sklearn.preprocessing import StandardScaler

# Select the numeric columns
num_cols = df_completed.select_dtypes(include=['int64', 'float64']).columns
# Remove the columns that we use to create the groups
num_cols = num_cols.drop(['Spotify_stream', 'Comments', 'Likes', 'Views'])

# Create the scaler
scaler = StandardScaler()

# Fit and transform the numeric columns
df_scaled = pd.DataFrame(scaler.fit_transform(df_completed[num_cols]), columns=num_cols)

# Add the 'Popularity_cat' column to the standardized DataFrame
df_scaled['popularity_group'] = df_completed['popularity_group']

import plotly.graph_objects as go

# Create a list of traces for each popularity_group
traces = []
for cat in df_scaled['popularity_group'].unique():
    data = df_scaled[df_scaled['popularity_group'] == cat][num_cols].mean().tolist()
    trace = go.Scatterpolar(
        r=data + data[:1],
        theta=num_cols.tolist() + [num_cols[0]],
        fill='toself',
        name=cat
    )
    traces.append(trace)

# Create the layout
layout = go.Layout(
    polar=dict(
        radialaxis=dict(visible=True),
    ),
    showlegend=True,
    title='Features Radar Plot by Popularity Group '
)

# Create the figure
fig = go.Figure(data=traces, layout=layout)

# Show the figure
fig.show()

#Save the Df  to a csv file as the data is clean and ready for the analysis and use in an app
df_completed_all_columns = df_completed.copy()
df_completed_all_columns.to_csv('spotify_youtube_df_completed.csv', index=False)

#Simple scatter plot (Danceability vs Energy) - we can see that there is a positive correlation between the two variables
ax = sns.scatterplot(data=df_completed, x='Danceability', y='Energy', hue='popularity_group', alpha=0.7)
ax.set_title('Danceability vs Energy')
plt.show()

# Spotify stream vs. Valence (positivity), by Album type
ax = sns.scatterplot(data=df_completed,
                x="Danceability",
                y="Tempo",
                hue="popularity_group")
ax.set_title('Danceability vs. Tempo')
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)
plt.show()

#Pairplot for relevant numerical columns - Data is beautiful :)
ax = sns.pairplot(df_completed, vars=['Liveness','Danceability',
                       'Loudness', 'Speechiness','Valence', 'Tempo', 'Acousticness', 'Views', 'Spotify_stream']
             ,hue='popularity_group', palette='rocket',kind='scatter', diag_kind='kde', height=2, corner=True,)
ax.fig.suptitle('Pairplot for relevant numerical columns', y=1.02, fontsize=30, fontweight='bold', ha='center')
ax.fig.subplots_adjust(top=0.95)
ax.fig.show()
plt.show()

#Correlation matrix dataframe
df_corr = df[['Liveness','Danceability',
                       'Loudness',
                       'Speechiness',
                       'Valence',
                       'Tempo',
                       'Acousticness',
                       'Views',
                       'Spotify_stream']].dropna().corr().round(2)

#Correlation matrix heatmap using seaborn
sns.heatmap(df_corr, annot=True)
plt.show()

"""# 4. Modeling

## 4.1 Feature Selection

Considerations for Feature Selection
- `Relevance` : How relevant is the feature to the target variable
- `Redundancy` : How much information does this feature provide that is not already provided by other features
- `Irrelevance` : How much noise does this feature add to the data
"""

df_completed.head()

#Lets drop the columns that are not relevant for the model
df_completed = df_completed.drop(['Artist', 'Album', 'Track', 'popularity_cluster','Views','Likes','Comments','Spotify_stream'], axis=1)
df_completed.info()

#Lets encode Album type , add it to the df and drop original column afterwards
df_encoded = pd.get_dummies(df_completed['Album_type'], prefix='Album')

#Concatenate the encoded columns to the dataframe
df_completed = pd.concat([df_completed, df_encoded], axis=1)

#Drop the original column
df_completed = df_completed.drop('Album_type', axis=1)

df_completed.describe()

#Feature Importance techniques
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler

# Create a RobustScaler instance
scaler = RobustScaler()

# Select the numeric columns
numeric_cols = df_completed.select_dtypes(include=np.number).columns

# Apply Scaling to the numeric columns
df_completed[numeric_cols] = scaler.fit_transform(df_completed[numeric_cols])
df_completed.head()

# Extract the features and target variables
X = df_completed.drop('popularity_group', axis=1)
y = df_completed['popularity_group']

# Calculate the mutual information score for each feature and the target
information_gain = SelectKBest(score_func=mutual_info_classif, k='all')
information_gain.fit(X, y)
information_gain_scores = information_gain.scores_

#Chi2 and Correlation are not suitable as the target variable is not continuous, the features are not categorical and some are not even ordinal

# Create a DataFrame with the feature names and information gain scores
feature_scores = pd.DataFrame({
    'Feature': X.columns,
    'Information Gain': information_gain_scores
})

# Sort the DataFrame by information gain score from high to low
feature_scores = feature_scores.sort_values(by='Information Gain', ascending=False)
print(feature_scores.head())

# Create a bar plot of the feature importances
feature_scores.plot(x='Feature', y='Information Gain', kind='bar', color='c')
plt.title('Feature Importance')
plt.show()

#Lets drop the columns that are not relevant for the model and leave key as its part of the song main features.
#df_completed = df_completed.drop(['Album_compilation', 'Album_album'], axis=1)

#I Have comented this part as the XgBoost is capable to interact with the minor changes on this features.

"""## 4.2. Model Selection and Evaluation

As the target variable is categorical let's try with 3 different models:
- Logistic Regression
- XGBoost
- SVM
"""

df_completed.groupby('popularity_group').count()

# Import libraries for modeling
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import RandomizedSearchCV

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE

# Preprocessing
encoder = LabelEncoder()
y = encoder.fit_transform(df_completed['popularity_group'])

# Divide the data into training and test sets with  SMOTE - k_neighbors=4 is set to include super popular as this just have 5 songs
smote = SMOTE(k_neighbors=4)
X_sm, y_sm = smote.fit_resample(X, y)

X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_sm, y_sm, test_size=0.2, random_state=42)

# Train a Logistic Regression model
lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')
lr.fit(X_train_sm, y_train_sm)
lr_preds = lr.predict(X_test_sm)

# Evaluate the model
print("Logistic Regression")
print(classification_report(y_test_sm, lr_preds))

#Train a SVM model
svm = SVC(decision_function_shape='ovo')
svm.fit(X_train_sm, y_train_sm)
svm_preds = svm.predict(X_test_sm)

# Evaluate the model
print("SVM")
print(classification_report(y_test_sm, svm_preds))

# Train a XGBoost model with RandomizedSearchCV
xgb = XGBClassifier(objective='multi:softmax')

# Define hyperparameter grid
param_grid = {
    'learning_rate': [0.1, 0.01, 0.001],
    'max_depth': [3, 5, 7, 10],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'n_estimators' : [100, 200, 500],
    'objective': ['multi:softmax']
}

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(xgb, param_grid, cv=3, scoring='accuracy')
random_search.fit(X_train_sm, y_train_sm)

# Best hyperparameters
print(random_search.best_params_)

# Predictions with the best model
xgb_preds = random_search.predict(X_test_sm)

# Evaluate the model
print("XGBoost with RandomizedSearchCV")
print(classification_report(y_test_sm, xgb_preds))

"""#### Based on the result we can clearly see that XGBoost is the winner, so lets keep working from there."""

# Plot confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns


cm = confusion_matrix(y_test_sm, xgb_preds)

# Plot confusion matrix
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d")
plt.title('Confusion matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()

# Plot feature importance
from xgboost import plot_importance
plot_importance(random_search.best_estimator_)
plt.show()

# Variables used to train the model
train_features = X_train_sm.columns.tolist()
train_features

#Lets Prepare our full dataset for teh Model
#Lets encode Album type , add it to the df and drop original column afterwards
df_encoded = pd.get_dummies(df_completed_all_columns['Album_type'], prefix='Album')

#Concatenate the encoded columns to the dataframe
df_completed = pd.concat([df_completed_all_columns, df_encoded], axis=1)

df_completed.head()

# Make predictions on the full dataset with XGBoost

index_error_count = 0

try:
    # Get probabilities for each class
    probabilities = random_search.best_estimator_.predict_proba(df_completed[train_features])

    # Add probabilities to the full dataset and print the head
    for i, class_name in enumerate(encoder.classes_):
        try:
            df_completed_all_columns[f'Probability_{class_name}'] = probabilities[:, i]
        except IndexError:
            index_error_count += 1
            df_completed_all_columns[f'Probability_{class_name}'] = 'NA'



except Exception as e:
    print("Error occurred: ", e)
    print("Se encontró un error durante la predicción. Asegúrese de que su modelo esté correctamente entrenado y que los datos de entrada sean válidos.")

print(f"Total index errors: {index_error_count}")

#Quick check of the outputs
df_completed_all_columns.head()

#I should have created this column a while ago ...
df_completed_all_columns['Track_Artist'] = df_completed_all_columns['Track'] + "_" + df_completed_all_columns['Artist']

"""#### Top 10 Mid popularity songs with a popability to be High popular"""

ax = df_completed_all_columns[(df_completed_all_columns['popularity_group'] == 'Medium-popularity')
                         & (df_completed_all_columns['Probability_High-popularity'] > 0)].sort_values(
                             by='Probability_High-popularity', ascending=False).head(10).plot.barh(x='Track_Artist', y='Probability_High-popularity', legend=False)
ax.set_title('Top 10 Mid-popularity songs with high probability to be high-popularity', pad=20, loc='left')
ax.invert_yaxis()
plt.show()

#Save the dataframe used to train the model to compare new  DS when transforming new datasets (Just for validation)
df_completed.to_csv('df_completed_used_for_train.csv', index=False)

# Save the data to a csv file
df_completed_all_columns.to_csv('spotify_youtube_df_completed_with_predictions.csv', index=False)

# Save the best model
from joblib import dump
dump(random_search.best_estimator_, 'xgboost_model.joblib')

# Save encoder for target variable
dump(encoder, 'label_encoder.joblib')

#Lets create a pre-processor pipeline to use in new data
import joblib
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler, OneHotEncoder

# Store by feature type
numeric_features = ['Danceability', 'Energy', 'Key', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo', 'Duration_ms']
categorical_features = ['Album_type']
boolean_features = ['Licensed', 'Official_video']

#Build preprocessing pipelines
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', RobustScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
#We don't need to do anything for boolean features


#Create the preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features),
        ('bool', 'passthrough', boolean_features)])

# Save the preprocessing pipeline
joblib.dump(preprocessor, 'preprocessor.pkl')

"""# 5. Key Learnings and Conclusions

## About the Data & Output

Music is beautiful; it has the power to transport us to different moments and places. It plays an important role alongside our sense of hearing and long-term memory.

**Artist**
- Ed Sheeran really knows how to make good music!
- Two out of the top 10 artists are Hispanic performers.
- CoComelon is a reference in children's music, worth exploring if you want to delve into this genre.

**Songs**
- "Despacito" is a worldwide case study as it has a melodic composition that plays interestingly with our brains. It is lively, simple, repetitive, and has a catchy rhythm.

**Factors that seem to make songs more or less popular**
In this part of the exercise, I like to use a [table](#Quick-table-view-by-popularity-cluster) to identify values and complement the analysis with a [radar diagram](#3.2.-Multi-variate-Analysis) that provides a more standardized and comparative visual (inspired by video games)(I'm checking why sometimes it's displayed and sometimes is not).

- Instrumentalness decreases as the popularity group increases, indicating that popular songs tend to have more vocal elements.
- Loudness levels decrease as the popularity group increases, suggesting a more balanced volume in popular songs.
- Danceability shows a notable trend, indicating that more popular songs tend to have higher danceability, potentially appealing to a larger audience.

**And the predictions?**
This exercise not only gives us a visual understanding of the probability of a song's popularity but also provides initial insights on how current songs can work on strategies to "increase" their popularity tier.

- Duration_ms, Loudness, and Tempo are among the top 3 representative variables for our model to identify which popularity group a song could belong to.

- If we zoom out of the exercise and recognize that there are more variables involved in the process of popularizing a song that we might not be considering, we could take this [top 10 songs](#Top-10-Mid-popularity-songs-with-a-popability-to-be-High-popular) and implement a boosting marketing strategy.

- If you're an artist looking to release a new song, how about first looking at existing songs with a similar flow and creating playlists with the most popular ones to enter the market?


### Limitations and Considerations:
This analysis may include certain biases and uncertainties as we do not know what other factors may be influencing the popularity of these songs. Let's consider factors such as language, wording of the lyrics, the music producer, as well as other elements like undersampling or oversampling of certain types of popular songs.
"""